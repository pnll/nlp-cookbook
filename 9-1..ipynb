{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all 20 categories:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "Sample Email:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample Target Category:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "print (\"20개 카테고리 전체 목록:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"\\n\")\n",
    "print (\"샘플 이메일:\")\n",
    "print (x_train[0])\n",
    "print (\"샘플 타깃 카테고리:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리에 사용 Used for pre-processing data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    except:\n",
    "        tokens = tokens\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed  = []\n",
    "for i in x_train:\n",
    "\tx_train_preprocessed .append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "\tx_test_preprocessed.append(preprocessing(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 벡터라이저vectorizer 구축\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 딥러닝 모듈\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "np.random.seed(1337) \n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Deep Layer Model building in Keras\n",
    "#del model\n",
    "# 케라스의 딥 레이어(심층) 모델 구축\n",
    "#del 모델\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 112s 10ms/step - loss: 1.9580\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 82s 7ms/step - loss: 0.5893\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.3023\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 80s 7ms/step - loss: 0.1762\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.1268\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 77s 7ms/step - loss: 0.0811\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 76s 7ms/step - loss: 0.0728\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 77s 7ms/step - loss: 0.0653\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 96s 9ms/step - loss: 0.0476\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0441\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 77s 7ms/step - loss: 0.0373\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 77s 7ms/step - loss: 0.0353\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 82s 7ms/step - loss: 0.0353\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0279\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0268\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0266\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0259\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 78s 7ms/step - loss: 0.0222\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 79s 7ms/step - loss: 0.0246\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 76s 7ms/step - loss: 0.0256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3f0139fa20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "딥 뉴럴 네트워크  - 학습 정확도:\n",
      "\n",
      "딥 뉴럴 네트워크  - 테스트 정확도:\n",
      "\n",
      "딥 뉴럴 네트워크  - 학습 분류 리포트Train Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       480\n",
      "          1       1.00      1.00      1.00       584\n",
      "          2       1.00      1.00      1.00       591\n",
      "          3       1.00      1.00      1.00       590\n",
      "          4       1.00      1.00      1.00       578\n",
      "          5       1.00      1.00      1.00       593\n",
      "          6       0.99      1.00      1.00       585\n",
      "          7       1.00      1.00      1.00       594\n",
      "          8       1.00      1.00      1.00       598\n",
      "          9       1.00      1.00      1.00       597\n",
      "         10       1.00      1.00      1.00       600\n",
      "         11       1.00      1.00      1.00       595\n",
      "         12       1.00      0.99      1.00       591\n",
      "         13       1.00      1.00      1.00       594\n",
      "         14       1.00      1.00      1.00       593\n",
      "         15       1.00      1.00      1.00       599\n",
      "         16       1.00      1.00      1.00       546\n",
      "         17       1.00      1.00      1.00       564\n",
      "         18       1.00      1.00      1.00       465\n",
      "         19       1.00      1.00      1.00       377\n",
      "\n",
      "avg / total       1.00      1.00      1.00     11314\n",
      "\n",
      "\n",
      "딥 뉴럴 네트워크  - 테스트 분류 리포트Test Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.75      0.79       319\n",
      "          1       0.66      0.75      0.70       389\n",
      "          2       0.74      0.66      0.70       394\n",
      "          3       0.65      0.70      0.68       392\n",
      "          4       0.77      0.78      0.78       385\n",
      "          5       0.81      0.76      0.78       395\n",
      "          6       0.77      0.83      0.80       390\n",
      "          7       0.79      0.90      0.84       396\n",
      "          8       0.95      0.91      0.93       398\n",
      "          9       0.90      0.91      0.91       397\n",
      "         10       0.91      0.98      0.94       399\n",
      "         11       0.92      0.89      0.91       396\n",
      "         12       0.81      0.65      0.72       393\n",
      "         13       0.87      0.84      0.85       396\n",
      "         14       0.87      0.92      0.90       394\n",
      "         15       0.83      0.91      0.87       398\n",
      "         16       0.77      0.87      0.82       364\n",
      "         17       0.96      0.87      0.91       376\n",
      "         18       0.77      0.62      0.68       310\n",
      "         19       0.60      0.61      0.61       251\n",
      "\n",
      "avg / total       0.81      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 예측\n",
    "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "print (\"\\n\\n딥 뉴럴 네트워크  - 학습 정확도:\"),(round(accuracy_score(y_train,y_train_predclass),3))\n",
    "print (\"\\n딥 뉴럴 네트워크  - 테스트 정확도:\"),(round(accuracy_score(y_test,y_test_predclass),3))\n",
    "\n",
    "print (\"\\n딥 뉴럴 네트워크  - 학습 분류 리포트Train Classification Report\")\n",
    "print (classification_report(y_train,y_train_predclass))\n",
    "\n",
    "print (\"\\n딥 뉴럴 네트워크  - 테스트 분류 리포트Test Classification Report\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
